NLP
Natural Language Processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and human language. It focuses on enabling computers to understand, interpret, and manipulate human language in a meaningful way


APPLICATIONS
Machine translation: Automatically translating text from one language to another.
Chatbots: Conversational AI agents that can interact with humans in a natural way.
Text summarization: Automatically generating summaries of text documents.
Sentiment analysis: Identifying the emotional tone of text data.
Information extraction: Extracting structured information from text documents.
Speech recognition: Converting spoken language into text.
Text generation: Generating text, such as product descriptions or marketing copy.

Disadvantages of NLP
Complexity and Data Requirements
Domain Specificity and Bias
Limited Common Sense and Explainability
Error Proneness and Computational Cost
Cultural and Linguistic Diversity

Challenges
Ambiguity: Words and phrases can have multiple meanings.
Context Understanding: Understanding context and nuances in language.
Data Sparsity: Limited availability of labeled data for training.
Language Evolution: Languages are dynamic and change over time.
Computational Complexity: Processing and understanding language requires significant computational resources.
Lack of Common Sense Reasoning: Understanding implicit meanings that humans inherently grasp

Tokenization
Tokenization means splitting text into meaningful unit words. There are sentence tokenizers as known tokens can be as small as characters or as sentences
Word Tokenization
Sentence Tokenization
Subword Tokenization -  breaks down words into smaller units, such as morphemes or characters
N-gram Tokenization - creating sequences of n consecutive tokens from the text

Why do we need feature extraction?
Feature Engineering is a very key part of Natural Language Processing algorithms and machines canâ€™t understand characters ,words or sentences hence we need to encode these words into some specific form of numerical in order to interact with algorithms or machines (vectorization, BOW, TF-IDF, n_grams)

Vectorization Methods:
Vectorization is the process of converting text data into numerical vectors that machine learning models can understand. Various methods include Bag of Words, TF-IDF, and Word Embeddings.

TF-IDF (Term Frequency-Inverse Document Frequency)
TF-IDF, which stands for Term Frequency-Inverse Document Frequency, is a widely used statistical technique in natural language processing to assess the importance of a word to a document in a corpus. It is used in a variety of applications, including search engines, text classification, and keyword extraction

Term Frequency (TF)
Term frequency (TF) measures how frequently a term word  appears in a document
TF = (number of times the term appears in the document) / (total number of words in the document)

Inverse Document Frequency (IDF)
Inverse document frequency (IDF) measures the rarity of a term across a corpus of documents
IDF = log(total number of documents in the corpus) / (number of documents containing the term)

TF-IDF Score
The TF-IDF score is calculated by multiplying the TF and IDF values. The higher the TF-IDF score, the more important the term is considered to be in the context of that document compared to other words.
TF-IDF = TF * IDF

Applications
Search engines: TF-IDF is used to rank documents in search results based on their relevance to a query.
Text classification: TF-IDF can be used to classify documents into different categories based on their content.
Keyword extraction: TF-IDF can be used to identify the most important keywords in a document.
Recommendation systems: TF-IDF can be used to recommend documents to users based on their past interests

Advantages 
Simple and easy to understand: TF-IDF is a relatively simple and easy-to-understand algorithm.
Effective for many NLP tasks: TF-IDF has been shown to be effective for a variety of NLP tasks.
Computationally efficient: TF-IDF is a computationally efficient algorithm

Disadvantages 
Sensitive to noisy data: TF-IDF can be sensitive to noisy data, such as misspelled words or irrelevant text.
Ignores word order: TF-IDF does not take into account the order of words in a document.
Does not consider semantic similarity: TF-IDF does not consider the semantic similarity of words.

Bag of Words:
Bag of words (BoW) is a simple and widely used technique for representing text documents in natural language processing (NLP). It is a method of converting text into a numerical representation It does this by creating a vector of word counts, where each element in the vector represents the number of times a particular word appears in the document.

Applications
Document Classification: Classifying documents into different categories based on their content.
Text Clustering: Grouping similar documents together based on their shared vocabulary.
Sentiment Analysis: Determining the sentiment of a text, such as positive, negative, or neutral.
Topic Modeling: Identifying the main topics or themes in a collection of documents

Advantages:
Simplicity: The bag of words model is easy to implement and understand.
Efficiency: It is computationally efficient to generate bag of words representations.
Effectiveness: It can be effective for tasks that rely on word frequency, such as document classification

Disadvantages:
Ignores Word Order: The bag of words model disregards the order of words, which can be important for capturing the meaning of a sentence.
Ignores Word Relationships: It does not consider the relationships between words, such as synonyms or antonyms.
Sensitivity to Noise: It can be sensitive to common words or stop words that don't carry much meaning

Word2Vec
Word2Vec is a versatile technique in (NLP) that transforms words into numerical vectors. These vectors, also known as word embeddings, capture the semantic and syntactic relationships between words by placing them in a high-dimensional space. By converting words into vectors, Word2Vec enables computers to grasp the meaning and context of words in a more human-like manner.

to calculate the distance between the vectors is 1 - cosine similarity

Essential Concepts in Word2Vec:
Vocabulary: The collection of all unique words in the training corpus.
Window Size: The number of neighboring words considered as context for a target word.
Word Embeddings: Vector representations of words, where words with similar meanings are positioned closer together in the vector space.

Two Primary Word2Vec Architectures:
Continuous Bag-of-Words (CBOW): Predictions of the target word  on the  context words.
Skip-Gram: Predict  context words from target word

Applications of Word2Vec:
Sentiment Analysis: Understanding the sentiment or emotional tone of text.
Topic Modeling: Identifying the main themes or topics in a collection of documents.
Machine Translation: Translating text from one language to another.
Text Classification: Categorizing text into predefined classes or labels

Advantages
Captures word relationships
Reduces dimensionality
Improves NLP performance

Disadvantages
Data sensitivity
Limited context
Computational complexity

CountVectorizer
CountVectorizer is a text-to-matrix transformation method commonly used in natural language processing (NLP) to convert a collection of text documents into a matrix of token counts. This matrix representation is then used as input for various machine learning algorithms, such as text classification, sentiment analysis, and document clustering.

How does CountVectorizer work?
Tokenization: Breaking down the text into individual words or tokens.
Conversion to lowercase: Converting all tokens to lowercase to ensure consistency.
Stop word removal: Removing common words that don't add much meaning, such as "the," "a," and "an."
Vocabulary creation: Building a vocabulary of unique words from the entire corpus.
Count matrix creation: Constructing a matrix where each row represents a document and each column represents a word in the vocabulary. The value in each cell is the frequency of the corresponding word in the corresponding document.

Advantages
Simple and efficient implementation.
Interpretable results due to the use of word counts.
Effective for capturing word frequency information.
Suitable for handling large text corpora

Disadvantages
Ignores word order and syntactic relationships.
Sensitive to the choice of stop words.
May not capture semantic similarities between words.

Parameters
Stop Words
Min_df
Max_df
Custom Preprocessing
Limiting Vocabulary Size